{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm \n",
    "import math\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_lexRank(sentences,tokens,threshold=0.1,epsilon=0.1):\n",
    "    freq_matrix = {}\n",
    "    ts=len(tokens)\n",
    "    for i,token in enumerate(tokens):\n",
    "        freq_table = {}\n",
    "        words = token\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        freq_matrix[i] = freq_table\n",
    "    tf_matrix = {}\n",
    "    c = 0\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[c] = tf_table\n",
    "        c = c+1\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "    idf_matrix = {}\n",
    "    idf_table = {}\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "\n",
    "        for word in f_table.keys():\n",
    "          if word not in idf_table:\n",
    "            idf_table[word] = math.log10(ts / float(word_per_doc_table[word]))\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "    matrix = np.zeros((ts, ts))\n",
    "    s = {}\n",
    "    for i in range(ts):\n",
    "      for j in range(ts):\n",
    "        u1 = tokens[i]\n",
    "        u2 = tokens[j]\n",
    "        common = list(set(u1) & set(u2))\n",
    "        d1=0\n",
    "        d2=0\n",
    "        n=0.0\n",
    "        for t in u1:\n",
    "          if t in tf_matrix[i]:\n",
    "            tf1=tf_matrix[i][t]\n",
    "          else:\n",
    "            tf1=0\n",
    "          if t in idf_table:\n",
    "            idf1=idf_table[t]\n",
    "          else:\n",
    "            idf1=0\n",
    "          d1+=(tf1*idf1)**2\n",
    "        for t in u2:\n",
    "          if t in tf_matrix[j]:\n",
    "            tf2=tf_matrix[j][t]\n",
    "          else:\n",
    "            tf2=0\n",
    "          if t in idf_table:\n",
    "            idf2=idf_table[t]\n",
    "          else:\n",
    "            idf2=0\n",
    "          d2+=(tf2*idf2)**2\n",
    "        for t in common:\n",
    "          if t in tf_matrix[i]:\n",
    "            tfc1=tf_matrix[i][t]\n",
    "          else:\n",
    "            tfc1=0\n",
    "          if t in tf_matrix[j]:\n",
    "            tfc2=tf_matrix[j][t]\n",
    "          else:\n",
    "            tfc2=0\n",
    "          if t in idf_table:\n",
    "            idf=idf_table[t]\n",
    "          else:\n",
    "            idf=0\n",
    "          n+=tfc1*tfc2*idf**2\n",
    "        if d1 > 0 and d2 > 0:\n",
    "          matrix[i][j] =  n / (math.sqrt(d1) * math.sqrt(d2))\n",
    "        else:\n",
    "          matrix[i][j] = 0.0\n",
    "    degrees = np.zeros((ts, ))\n",
    "    for i in range(ts):\n",
    "      for j in range(ts):\n",
    "        if matrix[i, j] > threshold:\n",
    "          matrix[i, j] = 1.0\n",
    "          degrees[i] += 1\n",
    "        else:\n",
    "          matrix[i, j] = 0\n",
    "    for i in range(ts):\n",
    "        for j in range(ts):\n",
    "            if degrees[i] == 0:\n",
    "                degrees[i] = 1\n",
    "\n",
    "            matrix[i][j] = matrix[i][j] / degrees[i]\n",
    "    transposed_matrix = matrix.T\n",
    "    p_vector = np.array([1.0 / ts] * ts)\n",
    "    lambda_val = 1.0\n",
    "\n",
    "    while lambda_val > epsilon:\n",
    "      next_p = np.dot(transposed_matrix, p_vector)\n",
    "      lambda_val = np.linalg.norm(np.subtract(next_p, p_vector))\n",
    "      p_vector = next_p\n",
    "    avg = np.sum(p_vector) / len(p_vector)\n",
    "    sentence_ids = []\n",
    "    for i in range(ts):\n",
    "      if(p_vector[i]>=avg):\n",
    "        sentence_ids.append(i)\n",
    "    summary = [sentences[i] for i in sentence_ids]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sentence(sentence, max_length=3000):\n",
    "    parts = []\n",
    "    for i in range(0, len(sentence), max_length):\n",
    "        part = sentence[i:i + max_length]\n",
    "        parts.append(part)\n",
    "\n",
    "    return parts\n",
    "\n",
    "def convertToHindi(sentences):\n",
    "    hindi = []\n",
    "    for sentence_array in sentences:\n",
    "        sentence = ''\n",
    "        hindiText=''\n",
    "        for i in range(len(sentence_array)):\n",
    "            sentence = sentence + sentence_array[i]\n",
    "        if len(sentence)>2000:\n",
    "            small_sentences = split_long_sentence(sentence, max_length=2000)\n",
    "            hindiPart = ''\n",
    "            for i in range(len(small_sentences)):\n",
    "                current_translate = GoogleTranslator(source='en', target='hi').translate(small_sentences[i])\n",
    "                hindiPart = hindiPart + current_translate\n",
    "        else:\n",
    "            hindiPart = GoogleTranslator(source='en', target='hi').translate(sentence)\n",
    "        hindiText=hindiText + hindiPart\n",
    "        hindi.append(hindiText)\n",
    "    return hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a2abd366024dbf849ce26de57a5f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_path=\"./data/summary/\"\n",
    "output_path=\"./data/summary_results/\"\n",
    "if os.path.exists(output_path) == False:\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "data = pd.read_csv(f'{input_path}data.csv')\n",
    "data = data.head(2)\n",
    "sentences = data['sentences'].apply(eval)\n",
    "sentences_english = data['sentences_english'].apply(eval)\n",
    "tokens = data['tokens'].apply(eval)\n",
    "tokens_english = data['english_tokens'].apply(eval)\n",
    "\n",
    "summary=[]\n",
    "summary_english=[]\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    summary.append(summarize_lexRank( sentences[i] ,tokens[i]))\n",
    "    summary_english.append(convertToHindi(summarize_lexRank(  sentences_english[i] ,tokens_english[i])))\n",
    "\n",
    "data['summary'] = summary\n",
    "data['summary_english'] = summary_english\n",
    "\n",
    "data.to_csv(f'{output_path}lexrank.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLDC",
   "language": "python",
   "name": "hldc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
